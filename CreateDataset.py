import os
import pandas as pd


def mergeDownloads(aggregation_level ='2dg', start_year = 2001, end_year = 2015,
                   keep_yr = None, sample = False, max_rows = 50000000, data_path = 'data', remove_oil = True):
    '''
    :param aggregation_level: HS product code aggregation ('2dg', '4dg' or '6dg')
    :param start_year: 1986 <= integer <= 2011 ending in 1 or 6
    :param end_year: 1990 <= integer <= 2015 ending in 0 or 5
    :param sample: False returns all data in a list of dataframes. True returns a single dataframe
    :param max_rows: maximum number of rows per dataframe
    :param data_path: location of the csv files generated by DownloadData.py
    :param remove_oil: by default mineral fuels, oils, distillation products are removed
    :return: sample dataframe or list of dataframes of length <= max_rows
    '''
    years = [str(yr) for yr in range(start_year, end_year, 5)]
    imports = []
    exports = []
    for file in os.listdir(data_path):
        file_start_yr = file[file.find('_',10)+1:file.find('_',10)+5]
        if ('imports' in file) and (aggregation_level in file) and (file_start_yr in years) and (file[0] != '.'):
            imports.append(file)
        if ('exports' in file) and (aggregation_level in file) and (file_start_yr in years) and (file[0] != '.'):
            exports.append(file)

    if len(imports) == 0:
        print 'No files matching those parameters found in %s' % data_path
        return

    if sample:
        # only use first 10 files
        if len(imports) > 10:
            imports = imports[:10]
        if len(exports) > 10:
            exports = exports[:10]

    headers = ['AltQuantity','CIFValue', 'FOBValue', 'GrossWeight', 'IsLeaf', 'NetWeight', 'TradeQuantity',
               'TradeValue', 'aggrLevel', 'cmdCode', 'cmdDescE', 'cstCode', 'cstDesc', 'estCode', 'motCode',
               'motDesc', 'period', 'periodDesc', 'pfCode', 'pt3ISO', 'pt3ISO2', 'ptCode', 'ptCode2',
               'ptTitle', 'ptTitle2', 'qtAltCode', 'qtAltDesc', 'qtCode', 'qtDesc', 'rgCode', 'rgDesc',
               'rt3ISO', 'rtCode', 'rtTitle', 'yr']

    keepColumns = ['rt3ISO','rtCode','rtTitle','pt3ISO','ptCode','ptTitle','aggrLevel',
                   'cmdCode','cmdDescE','yr','period','rgDesc','TradeValue']

    imports_df = pd.DataFrame(columns = keepColumns)
    for i,file in enumerate(imports):
        with open('%s/%s' % (data_path, file), mode = 'rU') as f:
            print 'appending %d/%d: %s' % (i+1, len(imports), file)
            try:
                df = pd.read_csv(f, sep = '\t', header=0, usecols=keepColumns)
            except ValueError:
                df = pd.read_csv(f, sep = '\t', header=None, names=headers, usecols=keepColumns)
            imports_df = imports_df.append(df)

    imports_df = imports_df.dropna(axis=0, how='all')
    int_cols = ['rtCode','ptCode','aggrLevel','cmdCode','yr']
    imports_df[int_cols] = imports_df[int_cols].apply(lambda x: x.astype(int))
    imports_df.rename(columns={'rt3ISO': 'toISO', 'rtCode': 'toCode', 'rtTitle': 'toTitle',
                                   'pt3ISO': 'fromISO', 'ptCode': 'fromCode', 'ptTitle': 'fromTitle'}, inplace=True)

    exports_df = pd.DataFrame(columns = keepColumns)
    for i,file in enumerate(exports):
        with open('%s/%s' % (data_path, file), mode = 'rU') as f:
            print 'appending %d/%d: %s' % (i+1, len(exports), file)
            try:
                df = pd.read_csv(f, sep = '\t', header = 0, usecols = keepColumns)
            except ValueError:
                df = pd.read_csv(f, sep='\t', header=None, names=headers, usecols=keepColumns)
            exports_df = exports_df.append(df)
    exports_df = exports_df.dropna(axis=0, how='any')
    exports_df[int_cols] = exports_df[int_cols].apply(lambda x: x.astype(int))
    exports_df.rename(columns={'rt3ISO': 'fromISO', 'rtCode': 'fromCode', 'rtTitle': 'fromTitle',
                                   'pt3ISO': 'toISO', 'ptCode': 'toCode', 'ptTitle': 'toTitle'}, inplace=True)

    for yr in range(start_year,end_year):
        imports_df = imports_df[imports_df.yr == yr]
        exports_df = exports_df[exports_df.yr == yr]

        print 'merging import and export datasets for %d' % yr
        comtrade = imports_df.merge(exports_df,how = 'outer',
                    on = ['fromISO','fromCode','fromTitle',
                          'toISO','toCode','toTitle',
                          'aggrLevel','cmdCode','cmdDescE',
                          'yr','period'])

        comtrade['source'] = comtrade.rgDesc_x
        comtrade.ix[pd.isnull(comtrade.source), 'source'] = comtrade.rgDesc_y
        comtrade.rename(columns={'TradeValue_x': 'reportedByImporter', 'TradeValue_y': 'reportedByExporter'}, inplace=True)
        comtrade['tradeValue'] = comtrade.reportedByImporter
        comtrade.ix[comtrade.source == 'Export', 'TradeValue'] = comtrade.reportedByExporter
        comtrade = comtrade.drop(['rgDesc_x','rgDesc_y'], axis = 1)

        remove = [  # countries/territories that don't interest me
            'AIA',  # Anguilla
            'ANT',  # Antartica
            'ATA',  # Antartica
            'BVT',  # Bouvet Island
            'IOT',  # British Indian Ociean Terr.
            'CXR',  # Christmas island
            'CCK',  # Cook islands
            'COK',  # Cocos is
            'FLK',  # Falklands
            'ATF',  # South Antartic Terr
            'HMD',  # Heard and McDonald Island
            'VAT',  # Vatican
            'MYT',  # Mayotte
            'MSR',  # Montserrat
            'NIU',  # Nieu
            'NFK',  # Norfolk Islands
            'PCN',  # Pitcairn
            'SHN',  # Saint Helena
            'SPM',  # Saint Pierre and Miguelon
            'SGS',  # South Georgia and the South Sandwich Islands
            'TKL',  # Tokelau
            'UMI',  # US Minor Outlying Islands
            'WLF',  # Walls and Futuna
            'ESH'  # Western Sahara
        ]
        keep_cols = ['fromCode', 'fromISO', 'toCode', 'toISO', 'yr', 'tradeValue', 'cmdCode']

        comtrade = comtrade[keep_cols]
        for iso in remove:
            comtrade = comtrade[comtrade.fromISO != iso]
            comtrade = comtrade[comtrade.toISO != iso]

        if remove_oil:
            digits = int(aggregation_level[0])
            comtrade['oil'] = comtrade.cmdCode.apply(lambda x: str(x).zfill(digits)[:2] == '27')
            comtrade = comtrade[comtrade.oil == False]
            comtrade = comtrade.drop('oil', axis=1)

        save_as = 'comtrade_%d_%d_%s' % (start_year,end_year,aggregation_level)
        if keep_yr!=None:
            save_as = 'comtrade_%d_%s' % (keep_yr,aggregation_level)
        if sample:
            save_as = 'comtrade_%d_%d_%s_sample' % (start_year,end_year,aggregation_level)
            if len(comtrade) <= max_rows:
                return comtrade, save_as
            return comtrade.sample(n=max_rows), save_as

        dataframes = []
        for start_row in range(0,len(comtrade),max_rows):
            end_row = start_row + max_rows
            if end_row < len(comtrade):
                dataframes.append(comtrade[start_row:end_row])
            else:
                dataframes.append(comtrade[start_row:])
        save_with_warning(comtrade, data_path, save_as)
        print 'saved %s' % save_as


def save(data, path, file_stub):
    if 'sample' in file_stub:
        data.to_csv('%s/%s.tsv' % (path,file_stub), index=None, sep='\t', encoding='utf-8')
        return
    for num, df in enumerate(data):
        df.to_csv('%s/%s_%d.tsv' % (path, file_stub, num), index=None, sep='\t', encoding='utf-8')

def save_with_warning(data, path, file_stub):
    for file in os.listdir(path):
        if file_stub in file:
            response = raw_input('Overwrite %s? y/n\n' % file_stub)
            if response == 'y':
                if 'sample' in file_stub:
                    save(data, path, file_stub)
            break
    save(data, path, file_stub)

if __name__ == "__main__":

    mergeDownloads(aggregation_level ='4dg', start_year = 2011, end_year = 2015, data_path='/media/peter/HDD/UN COMTRADE')
